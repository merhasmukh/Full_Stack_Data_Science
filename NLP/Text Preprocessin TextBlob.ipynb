{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob: Simplified Text Processing \n",
    "\n",
    "TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation,and more.\n",
    "\n",
    "TextBlob stands on the giant shoulders of NLTK and pattern, and plays nicely with both.\n",
    "\n",
    "## Features\n",
    "- Noun phrase extraction\n",
    "- Part-of-speech tagging\n",
    "- Sentiment analysis\n",
    "- Classification (Naive Bayes, Decision Tree)\n",
    "- Language translation and detection powered by Google Translate\n",
    "- Tokenization (splitting text into words and sentences)\n",
    "- Word and phrase frequencies\n",
    "- Parsing\n",
    "- n-grams\n",
    "- Word inflection (pluralization and singularization) and lemmatization\n",
    "- Spelling correction\n",
    "- Add new models or languages through extensions\n",
    "- WordNet integration\n",
    "\n",
    "## Installation\n",
    "\n",
    "### Installing/Uploading from the PyPi\n",
    "     \n",
    "      pip install textblob\n",
    "      python -m textblob.download_corpora\n",
    "\n",
    "This will install TextBlob and download the necessary NLTK corpora. If you need to change the default download directory set the NLTK_DATA environment variable.\n",
    "\n",
    "### Downloading the minimum corpora:\n",
    "\n",
    "If you only intend to use TextBlob’s default models (no model overrides), you can pass the lite argument. This downloads only those corpora needed for basic functionality.\n",
    "\n",
    "      python -m textblob.download_corpora lite\n",
    "      \n",
    "### Installing with Conda\n",
    "\n",
    "***Note:*** Conda builds are currently available for Mac OSX only.\n",
    "\n",
    "TextBlob is also available as a conda package. To install with conda, run\n",
    "\n",
    "     conda install -c https://conda.anaconda.org/sloria textblob\n",
    "     python -m textblob.download_corpora\n",
    "     \n",
    "### Python\n",
    "\n",
    "TextBlob supports Python >=2.7 or >=3.4.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "TextBlob depends on NLTK3. NLTK will be installed automatically when you run\n",
    "     \n",
    "     pip install textblob\n",
    "     \n",
    "     \n",
    "TextBlob aims to provide access to common text-processing operations through a familiar interface. You can treat **TextBlob** objects as if they were Python strings that learned how to do Natural Language Processing.\n",
    "\n",
    "### Create a TextBlob\n",
    "\n",
    "First, the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /home/tech/.local/lib/python3.10/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: joblib in /home/tech/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/tech/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2022.8.17)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /home/tech/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our first **TextBlob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextBlob(\"I love Natural Language Processing, not you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['I', 'love', 'Natural', 'Language', 'Processing', 'not', 'you'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "\n",
    "data.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"I love Natural Language Processing, not you! .\"),\n",
       " Sentence(\"i am hasmukh mer\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = TextBlob(\"I love Natural Language Processing, not you! . i am hasmukh mer\")\n",
    "data2.sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love Natural Language Processing, not you! .\n",
      "i am hasmukh mer\n"
     ]
    }
   ],
   "source": [
    "for i in data2.sentences:\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech(POS) Tagging\n",
    "\n",
    "Parts-of-speech tags can be accessed through the **tags** property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('love', 'VBP'),\n",
       " ('Natural', 'JJ'),\n",
       " ('Language', 'NNP'),\n",
       " ('Processing', 'NNP'),\n",
       " ('not', 'RB'),\n",
       " ('you', 'PRP')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3 = TextBlob(\"I love Natural Language Processing, not you!\")\n",
    "data3.tags\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Phrase Extraction\n",
    "\n",
    "Similarly, noun phrases are accessed through the **noun_phrases** property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/tech/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WordList(['hasmukh mer', 'machine learning engineer'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown') # required for noun\n",
    "data4 = TextBlob(\"I am hasmukh mer and working as a machine learning engineer\")\n",
    "data4.noun_phrases\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "The sentiment property returns a named tuple of the form Sentiment(polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.06545454545454545, subjectivity=0.4381818181818182)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5 = TextBlob(\"i like very much to learn new things everyday\")\n",
    "data5.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=1.0, subjectivity=1.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data6 = TextBlob(\"i am very happy\")\n",
    "data6.sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Word Inflection and lemmatization\n",
    "\n",
    "  Each word in the **TextBlob.words** or **Sentence.words** is a **Word** object(a subclass of unicode) with useful methods, e.g. for word inflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Use', '4', 'spaces', 'per', 'indentation', 'level'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data7 = TextBlob('Use 4 spaces per indentation level')\n",
    "\n",
    "data7.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use\n",
      "4\n",
      "space\n",
      "per\n",
      "indentation\n",
      "level\n"
     ]
    }
   ],
   "source": [
    "for i in data7.words:\n",
    "    print(i.lemmatize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'space'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data7.words[2].singularize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uses'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data7.words[0].pluralize()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the definitions for each synset via the **definitions** property or the **define()** method, which can also take an optional part-of-speech(pos) argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the vertical dimension of extension; distance from the base of something to the top',\n",
       " 'the highest level or degree attainable; the highest stage of development',\n",
       " '(of a standing person) the distance from head to foot',\n",
       " \"elevation especially above sea level or above the earth's surface\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "# Word(\"machine learning\").definitions\n",
    "Word(\"height\").definitions\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction\n",
    "\n",
    "For correcting the words you can use **correct()** method to attempt spelling correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you pronounce machine learning engineer?\n"
     ]
    }
   ],
   "source": [
    "g = TextBlob('can you pronounce machina learnig engineer?')\n",
    "print(g.correct())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word objects have a **spellcheck() Word.spellcheck()** , this method that returns a list of (word, confidence) tuples with spelling suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machine', 0.975), ('gachina', 0.025)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "k = Word('machina')\n",
    "k.spellcheck()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This spelling correction is based on the Peter Norvig's \"How to Write a Spelling Corrector\", as implemented in the pattern library. It is about 70% accurate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Word and Noun Phrase Frequencies\n",
    "\n",
    "There are two ways to get the frequency of a word or noun phrase in the **TextBlob**\n",
    "\n",
    "The first one is through the word_counts dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = TextBlob('She sales sea shells at the sea shore.')\n",
    "\n",
    "sent.word_counts['sea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2 = TextBlob('She sales sea shells at the sea shore.')\n",
    "\n",
    "sent2.word_counts['Sea']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you access the frequencies this way, the search will not be case sensitive, and words that are not found will have a frequency of 0.\n",
    "\n",
    "The second way is to use the count() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2 = TextBlob('She sales sea shells at the sea shore.')\n",
    "sent2.words.count('sea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent3 = TextBlob('She sales sea shells at the sea shore.')\n",
    "sent3.words.count('Sea')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify whether or not the search should be case-sensitive (default is False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 = TextBlob('She sales sea shells at the sea shore.')\n",
    "\n",
    "sent4.words.count('Sea', case_sensitive=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 = TextBlob('She sales sea shells at the sea shore.')\n",
    "\n",
    "sent4.words.count('Sea', case_sensitive=False) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation and Language Detection\n",
    "\n",
    "TextBlobs can be translated between languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am hasmukh mer a machine learning engineer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextBlob(\"मैं हसमुख मेर एक मशीन लर्निंग इंजीनियर हूं\")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trns = TextBlob('I am hasmukh mer a machine learning engineer')\n",
    "print(trns)\n",
    "trns.translate(from_lang=\"es\",to='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"我是一名机器学习工程师\")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trns.translate(from_lang=\"es\",to='zh-CN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I am a machine learning engineer\")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_blob = TextBlob(\"我是一名机器学习工程师\")\n",
    "chinese_blob.translate(from_lang=\"zh-CN\", to='en')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlobs Are Like Python Strings!\n",
    "\n",
    "You can use Python’s substring syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"hasmukh mer a machine learning engineer\")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data8=TextBlob(\"hasmukh mer a machine learning engineer\")\n",
    "data8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"hasmukh m\")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data8[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data8.find(\"mer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can make comparisons between TextBlobs and strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_blob = TextBlob('aple')\n",
    "s_blob = TextBlob('samsumg')\n",
    "\n",
    "a_blob < s_blob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can concatenate and interpolate TextBlobs and strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"aple and samsumg\")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_blob + ' and ' + s_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aple and samsumg'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{0} and {1}\".format(a_blob,s_blob)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "\n",
    "The **TextBlob.ngrams()** method returns a list of tuples of n successive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is', 'better']),\n",
       " WordList(['is', 'better', 'than']),\n",
       " WordList(['better', 'than', 'never'])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"Now is better than never.\")\n",
    "blob.ngrams(n=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Start and End Indices of Sentences\n",
    "\n",
    "Use sentence.start and sentence.end to get the indices where a sentence starts and ends within a **TextBlob.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************\n",
      "i am hasmukh mer.\n",
      "---- Starts at index 0, Ends at index 17\n",
      "************\n",
      "machine learning engineer\n",
      "---- Starts at index 18, Ends at index 43\n"
     ]
    }
   ],
   "source": [
    "sent5=TextBlob(\"i am hasmukh mer. machine learning engineer\")\n",
    "for k in sent5.sentences:\n",
    "    print(\"************\")\n",
    "    print(k)\n",
    "   \n",
    "    print(\"---- Starts at index {}, Ends at index {}\".format(k.start, k.end))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start building the Text Classification system\n",
    "\n",
    "The __textblob.classifiers__ module makes it simple to create custom classifiers.\n",
    "\n",
    "As an example, let’s create a custom sentiment analyzer.\n",
    "\n",
    "## Loading Data and Creating a Classifier\n",
    "\n",
    "First we’ll create some training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\n",
    "     ('I love this sandwich.', 'pos'),\n",
    "     ('this is an amazing place!', 'pos'),\n",
    "     ('I feel very good about these beers.', 'pos'),\n",
    "     ('this is my best work.', 'pos'),\n",
    "     (\"what an awesome view\", 'pos'),\n",
    "     ('I do not like this restaurant', 'neg'),\n",
    "     ('I am tired of this stuff.', 'neg'),\n",
    "     (\"I can't deal with this\", 'neg'),\n",
    "     ('he is my sworn enemy!', 'neg'),\n",
    "     ('my boss is horrible.', 'neg')\n",
    "]\n",
    "\n",
    "test = [\n",
    "     ('the beer was good.', 'pos'),\n",
    "     ('I do not enjoy my job', 'neg'),\n",
    "     (\"I ain't feeling dandy today.\", 'neg'),\n",
    "     (\"I feel amazing!\", 'pos'),\n",
    "     ('Gary is a friend of mine.', 'pos'),\n",
    "     (\"I can't believe I'm doing this.\", 'neg')\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll create a Naive Bayes classifier, passing the training data into the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data from Files\n",
    "\n",
    "You can also load data from common file formats including CSV, JSON, and TSV.\n",
    "\n",
    "CSV files should be formatted like so:\n",
    "\n",
    "      I love this sandwich.,pos\n",
    "      This is an amazing place!,pos\n",
    "      I do not like this restaurant,neg\n",
    "      \n",
    "JSON files should be formatted like so:\n",
    "\n",
    "[\n",
    "    {\"text\": \"I love this sandwich.\", \"label\": \"pos\"},\n",
    "    {\"text\": \"This is an amazing place!\", \"label\": \"pos\"},\n",
    "    {\"text\": \"I do not like this restaurant\", \"label\": \"neg\"}\n",
    "]\n",
    "\n",
    "You can then pass the opened file into the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('train.json', 'r') as fp:\n",
    "#     cl = NaiveBayesClassifier(fp, format=\"json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Text\n",
    "\n",
    "Call the *classify(text)* method to use the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.classify(\"This is an amazing library!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the label probability distribution with the *prob_classify(text)* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dist = cl.prob_classify(\"I am suffering from cough and cold.\")\n",
    "prob_dist.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(prob_dist.prob(\"neg\"), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(prob_dist.prob(\"pos\"), 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying TextBlobs\n",
    "\n",
    "Another way to classify text is to pass a classifier into the constructor of TextBlob and call its *classify()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"Alcohol is good. But the hangover is horrible.\", classifier=cl)\n",
    "blob.classify()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this approach is that you can classify sentences within a **TextBlob**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol is good.\n",
      "pos\n",
      "But the hangover is horrible.\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "for b in blob.sentences:\n",
    "    print(b)\n",
    "    print(b.classify())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifiers\n",
    "\n",
    "To compute the accuracy on our test set, use the **accuracy(test_data)** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the show_informative_features() method to display a listing of the most informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            contains(my) = True              neg : pos    =      1.7 : 1.0\n",
      "            contains(an) = False             neg : pos    =      1.6 : 1.0\n",
      "             contains(I) = False             pos : neg    =      1.4 : 1.0\n",
      "             contains(I) = True              neg : pos    =      1.4 : 1.0\n",
      "            contains(my) = False             pos : neg    =      1.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "cl.show_informative_features(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Classifiers with New Data\n",
    "\n",
    "Use the update(new_data) method to update a classifier with new training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = [('She is my best friend.', 'pos'),\n",
    "           (\"I'm happy to have a new friend.\", 'pos'),\n",
    "           (\"Stay thirsty, my friend.\", 'pos'),             \n",
    "           (\"He ain't from around here.\", 'neg')]\n",
    "\n",
    "cl.update(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
