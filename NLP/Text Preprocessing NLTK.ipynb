{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFKabdVzPAnQRwnG3Zmii1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Text preprocessing Using NLTK Python Library"],"metadata":{"id":"6ZSU2xaky_9a"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z1dq3RBBnQdQ","executionInfo":{"status":"ok","timestamp":1675233010269,"user_tz":-330,"elapsed":434,"user":{"displayName":"Mr H","userId":"03094714360012541181"}},"outputId":"efbcbf79-9c86-48dd-d986-c07c1af37bdb"},"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'AM', 'hasmukh', 'mer', ',', 'a', 'machine', 'learning', 'engineer']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["# word tokenization\n","\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","nltk.download('punkt')\n","\n","# Punkt Sentence Tokenizer\n","\n","# This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used.\n","\n","# The NLTK data package includes a pre-trained Punkt tokenizer for English.\n","\n","str1=\"i AM hasmukh mer, a machine learning engineer\"\n","\n","\n","print(word_tokenize(str1))"]},{"cell_type":"code","source":["# sentence tokenization\n","\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","nltk.download('punkt')\n","\n","str2=\"i AM hasmukh mer, a machine learning engineer. i am from gujrat\"\n","\n","\n","print(sent_tokenize(str2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFyjijLVosDT","executionInfo":{"status":"ok","timestamp":1675233010643,"user_tz":-330,"elapsed":10,"user":{"displayName":"Mr H","userId":"03094714360012541181"}},"outputId":"d63c3f0f-c579-4fdf-ab59-1c676332866c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['i AM hasmukh mer, a machine learning engineer.', 'i am from gujrat']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["### Stemming\n","\n","From Stemming we will process of getting the root form of a word. Root or Stem is the part to which inflextional affixes(like -ed, -ize, etc) are added. We would create the stem words by removing the prefix of suffix of a word. So, stemming a word may not result in actual words.\n","\n","For Example: Mangoes ---> Mango\n","\n","             Boys ---> Boy\n","             \n","             going ---> go\n","             \n","             \n","If our sentences are not in tokens, then we need to convert it into tokens. After we converted strings of text into tokens, then we can convert those word tokens into their root form. These are the Porter stemmer, the snowball stemmer, and the Lancaster Stemmer. We usually use Porter stemmer among them."],"metadata":{"id":"utR0HrLAzQ9j"}},{"cell_type":"code","source":["# Stemming\n","\n","\n","# import these modules\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","ps = PorterStemmer()\n","\n","str3=\"i AM hasmukh mer, a machine learning engineer. i am from gujrat\"\n","\n","str3=word_tokenize(str3)\n","\n","stem_words=[ps.stem(i) for i in str3] # list comprehension\n","# for i in str3:\n","#   print(ps.stem(i))\n","\n","print(stem_words)\n","  \n"],"metadata":{"id":"a_Fd5SY4n9ly","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675233010644,"user_tz":-330,"elapsed":8,"user":{"displayName":"Mr H","userId":"03094714360012541181"}},"outputId":"6d948c94-2348-40e3-da84-be0e1f3802ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'am', 'hasmukh', 'mer', ',', 'a', 'machin', 'learn', 'engin', '.', 'i', 'am', 'from', 'gujrat']\n"]}]},{"cell_type":"markdown","source":["### Lemmatization\n","\n","As stemming, lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. Because of the use of lemmatization we will get the valid words. In NLTK(Natural language Toolkit), we use WordLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization.So, we added pos(parts-of-speech) as a parameter. "],"metadata":{"id":"AGyS_hYizN8r"}},{"cell_type":"code","source":["#lemmitization\n","\n","# import these modules\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt') # required for word_tokenize\n","nltk.download('wordnet') # required for lemmatize\n","nltk.download('omw-1.4') # required for lemmatize\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","str4=\"i AM hasmukh mer, a machine learning engineer. i am from gujrat\"\n","\n","str4=word_tokenize(str4)\n","\n","lemma_words=[lemmatizer.lemmatize(i) for i in str4] # list comprehension\n","lemma_words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1MZXqyMnqgM5","executionInfo":{"status":"ok","timestamp":1675233246615,"user_tz":-330,"elapsed":2,"user":{"displayName":"Mr H","userId":"03094714360012541181"}},"outputId":"06eda078-1122-4861-a125-48b8c85554ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["['i',\n"," 'AM',\n"," 'hasmukh',\n"," 'mer',\n"," ',',\n"," 'a',\n"," 'machine',\n"," 'learning',\n"," 'engineer',\n"," '.',\n"," 'i',\n"," 'am',\n"," 'from',\n"," 'gujrat']"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["### Remove default stopwords:\n","\n","Stopwords are words that do not contribute to the meaning of the sentence. Hence, they can be safely removed without causing any change in the meaning of a sentence. The NLTK(Natural Language Toolkit) library has the set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens."],"metadata":{"id":"W6Bra5ZlzUu0"}},{"cell_type":"code","source":["#remove stopwords\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords') # required for stopwords\n","nltk.download('punkt') # required for word_tokenize\n","str5=\"i AM hasmukh mer, a machine learning engineer. i am from gujrat\"\n","stopwords=stopwords.words('english')\n","str5=word_tokenize(str5)\n","\n","lemma_words=[i for i in str5 if i.lower() not in stopwords] # list comprehension\n","lemma_words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"djQSxv7arTBx","executionInfo":{"status":"ok","timestamp":1675234409886,"user_tz":-330,"elapsed":8,"user":{"displayName":"Mr H","userId":"03094714360012541181"}},"outputId":"67d73a7b-4e57-47f0-ff5e-2211b7fe5e5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["['hasmukh', 'mer', ',', 'machine', 'learning', 'engineer', '.', 'gujrat']"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["print(stopwords)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wAYlTKL0syW-","executionInfo":{"status":"ok","timestamp":1675234442253,"user_tz":-330,"elapsed":420,"user":{"displayName":"Mr H","userId":"03094714360012541181"}},"outputId":"164b2950-a9b3-42fe-ea17-9ebf6a627d45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}]},{"cell_type":"markdown","source":["\n","The pos(parts of speech) explain you how a word is used in a sentence. In the sentence, a word have different contexts and semantic meanings. The basic natural language processing(NLP) models like bag-of-words(bow) fails to identify these relation between the words. \n","For that we use pos tagging to mark a word to its pos tag based on its context in the data. Pos is also used to extract rlationship between the words.\n"],"metadata":{"id":"DmpEZi9Syq_y"}},{"cell_type":"code","source":["\n","# importing tokenize library\n","import nltk\n","from nltk.tokenize import word_tokenize \n","from nltk import pos_tag \n","nltk.download('averaged_perceptron_tagger') # required for pos tag\n","nltk.download(\"punkt\")# required for word_tokenize\n","  \n","# convert text into word_tokens with their tags \n","def pos_tagg(text): \n","    word_tokens = word_tokenize(text) \n","    return pos_tag(word_tokens) \n","  \n","pos_tagg('are you hasmukh? a machine learning engineer') "],"metadata":{"id":"GloVmas2sYqR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675402818621,"user_tz":-330,"elapsed":8,"user":{"displayName":"Mr H","userId":"03094714360012541181"}},"outputId":"92f84dee-d321-4190-d4ca-9bda374727e0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["[('are', 'VBP'),\n"," ('you', 'PRP'),\n"," ('hasmukh', 'VB'),\n"," ('?', '.'),\n"," ('a', 'DT'),\n"," ('machine', 'NN'),\n"," ('learning', 'NN'),\n"," ('engineer', 'NN')]"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["In the above example NNP stands for Proper noun, PRP stands for personal noun, IN as Preposition. We can get all the details pos tags using the Penn Treebank tagset"],"metadata":{"id":"2strkujGypIb"}},{"cell_type":"code","source":["# downloading the tagset  \n","nltk.download('tagsets') \n","  \n","# extract information about the tag \n","nltk.help.upenn_tagset('PRP')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"msc9vH8AyP6G","executionInfo":{"status":"ok","timestamp":1675402890479,"user_tz":-330,"elapsed":9,"user":{"displayName":"Mr H","userId":"03094714360012541181"}},"outputId":"0de87a77-9dc5-47df-f4ec-ea55b8dfb920"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["PRP: pronoun, personal\n","    hers herself him himself hisself it itself me myself one oneself ours\n","    ourselves ownself self she thee theirs them themselves they thou thy us\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package tagsets to /root/nltk_data...\n","[nltk_data]   Unzipping help/tagsets.zip.\n"]}]},{"cell_type":"code","source":["# extract information about the tag \n","nltk.help.upenn_tagset('NN')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XwX3-Bgcy0b9","executionInfo":{"status":"ok","timestamp":1675402912148,"user_tz":-330,"elapsed":6,"user":{"displayName":"Mr H","userId":"03094714360012541181"}},"outputId":"48613d8f-1a0e-4cb8-e286-09adad9a9f8f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["NN: noun, common, singular or mass\n","    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n","    investment slide humour falloff slick wind hyena override subhumanity\n","    machinist ...\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WWtKZpXJy5qN"},"execution_count":null,"outputs":[]}]}